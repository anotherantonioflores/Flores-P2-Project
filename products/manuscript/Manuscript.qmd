---
title: "Predicting Customer Behavior Using a Portuguese Financial Institution Dataset "
subtitle: ""
author: Antonio Flores
date: "`r Sys.Date()`"
format:
  docx:
    toc: false
    number-sections: true
    highlight-style: github
bibliography: ../../assets/dataanalysis-references.bib
csl: ../../assets/apa.csl
---

```{r, echo=FALSE, message=FALSE, warning = FALSE}
library(here)
library(knitr)
```

# Abstract

Financial institutions spend billions of dollars on their marketing teams every year, with a positive trend observed in 2023 for the largest and smallest banks (@akins2024). With the need for efficient marketing becoming more and more important, machine learning models are viable tools for refining marketing strategies. This project will seek to identify a model for predicting consumer financial behavior using a dataset from a Portuguese bank by comparing the accuracy and predictive power of different machine learning models.

{{< pagebreak >}}

# Introduction

## General Background Information

In 2022, JPMorgan Chase & Co. spent \$3.9 billion (US) dollars on Marketing, leading other financial institutions in this category. Recent reports have shown that this has only grown in past years (@jpmorgan2023). Providing businesses with a model that will allow prioritization of consumers and/or demographics has great potential in improving resource management and future marketing campaigns, as well as increasing efficient spending. This particular project will utilize Classification Prediction. Unlike Regression Prediction which will attempt to predict a continuous value (e.g., x amount of dollars, x amount of cells), Classification Prediction seeks to train a model that can correctly predict a classification (e.g., True/False, Success/Failure) based on the given predictors(@kuhn2018). In our case, our y variable or response variable is a binary variable, Yes/No, answering whether or not a customer subscribed to a term deposit.

## Description of data and data source

The data was donated on 2/13/2012. It was collected from phone call marketing campaigns performed by a Portuguese banking institution.I have accessed this data from the UC Irvine Machine Learning Repository.

There are 45,212 records,17 columns/variables which include: age, marital status, job, education, details related to the phone call, as well as answers related to questions about past credit history. Additionally, as mentioned, the classification variable is whether or not the person subscribed to a term deposit.

## Questions/Hypotheses to be addressed

The research question I plan to address with my analysis is: which features or combination of features are the best predictors of consumers making a deposit? The desired output of this analysis is a model which allows a financial institution to better prioritize/make decisions regarding future marketing campaigns. I plan to investigate all demographic variables, with a specific focus on job type, education and age.

{{< pagebreak >}}

# Methods

## Data aquisition

The dataset for this project was retrieved from UCI ML Repository in CSV form. Additionally, I created a codebook based on data from the same source.

## Data import and cleaning

### Reading in the Data

```{r}
#| label: tbl-summarytable1
#| tbl-cap: "Data Snapshot of first 5 Variables"
#| echo: FALSE
data_location <- here::here("data","raw-data","bank-full.csv")
raw=read.csv(data_location, header = TRUE, sep=";")
kable(head(raw[1:5]))
```

```{r}
#| label: tbl-summarytable2
#| tbl-cap: "Data Snapshot of next 5 Variables"
#| echo: FALSE
kable(head(raw[6:10]))

```

### Dimensions:

Rows: 45, 211\
Columns: 17 {{< pagebreak >}}

### Describing data

```{r, echo=FALSE, message=FALSE, warning = FALSE}
data_location <- here::here("results","tables","describetbl1.rds")
describe1 <- readRDS(data_location)
describe1
```

### Cleaning Data

There were several different data cleaning methods that were experimented with in order to find the best way to prepare the data for modeling. First, I converted several variables to factors as they had been read in as character variables, or strings. Next I tried to use the DummyVars tool to convert every categorical variable to a dummy value but this made modeling difficult due to a large amount of binary variables. Instead I proceeded with converting the categorical variables to numeric values while maintaining their factor status. I also created a dataset with these numeric values that were not factors. These were the two primary dataset I used for my modeling. Finally I created a dataset stripping the categorical values of their attributes, this was solely created to allow for my corrplot to work.

### Removing Initial Predictors

I removed the 'Duration' variable due to a warning I discovered on the site hosting this dataset initially: "Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model."(@misc_bank_marketing_222) Due to this new information I decided to remove all the variables related to this, which included 'campaign', 'pdays', 'previous', and 'poutcome.' Bringing our new total number of predictors to 11.

{{< pagebreak >}}

# Results

## Exploratory Data Analysis

Peng and Matsui define EDA as "the process of exploring your data","\[including\] examining the structure and components of your dataset, the distributions of individual variables, and the relationships between two or more variables"(@peng2018).

In accordance with this definition, we will be exploring the data using different charts to identify outliers, abnormalities, relationships, and the general shape and feel of different variables.

@fig-result1 shows the distribution of the 'Age' variable.

```{r}
#| label: fig-result1
#| echo: FALSE
knitr::include_graphics(here("results","figures","age-distribution.png"))
```

While this shows a slight skew to the right, if we account for the extreme values, this data seems to be fairly normally distributed. {{< pagebreak >}}

@fig-result2 shows the distribution of the 'Education' variable

```{r}
#| label: fig-result2
#| fig-cap: "Education level Bar Chart"
#| echo: FALSE
knitr::include_graphics(here("results","figures","education-barchart.png"))
```

@fig-result3 shows the most common 'Job' types in our data

```{r}
#| label: fig-result3
#| fig-cap: "Most Common Job Types"
#| echo: FALSE
knitr::include_graphics(here("results","figures","job-barchart.png"))
```

{{< pagebreak >}}

@fig-result4 shows a scatter plot with Age and bank account balance as the (x,y) values, stratified by marital status.

```{r}
#| label: fig-result4
#| fig-cap: "Scatterplot of Age and Balance"
#| echo: FALSE
knitr::include_graphics(here("results","figures","age-balance-stratified.png"))
```

@fig-result5 shows a barplot of the most common days of the month to record a positive outcome.

```{r}
#| label: fig-result5
#| echo: FALSE
knitr::include_graphics(here("results","figures","days-yes.png"))
```

While there doesn't appear to be a significant trend, we can clearly see that the 30th of the month stands out as being a day of interest, especially compared to other days of the months. 
{{< pagebreak >}}

@fig-result6 shows potential correlation between different variables.

```{r}
#| label: fig-result6
#| fig-cap: "Correlation Matrix"
#| echo: FALSE
knitr::include_graphics(here("results","figures","corplot.png"))
```

The biggest takeaways from our Exploratory Data Analysis is that most variables are fairly normally distributed due to the size of the data, and most variable are not correlated to each other, with one exception ('Age' and 'Marital') {{< pagebreak >}}

## Basic statistical analysis

Before I began with the machine learning analysis, I sought to test the variables of interest in their significance of affecting the response variable. As mentioned previously, our response variable is a binary value (Yes/No), which means that we need to use a logistic regression model instead of a linear regression model(@kuhn2018).

Below are the results of the logistic model fit with all variables as predictors

```{r}
#| label: basicmodel1
#| echo: FALSE
data_location <- here::here("results","tables","fullmodel.rds")
full1 <- readRDS(data_location)
kable(full1)
```

As expected, all predictors are very significant as determined by the very small p-values. We will proceed to the Machine Learning portion of this project.

## Machine Learning Modeling

I chose to utilize the following models: the Multivariate Adaptive Regression Splines model (MARS), K-Nearest Neighbors model (KNN), Logistic Regression model, and RandomForest models to determine the best prediction model for this project.

### Background on Chosen Models

**MARS:**\
The MARS model has some similarities to both Neural Networks and the Partial Least Squares model, but the distinguishing feature is a use of multiple 'splines' to create a "piecewise linear model" with multiple features modeling a separate part of the data (@kuhn2018).

```{r}
#| label: fig-result999
#| fig-cap: "MARS Model example from MiniTab"
#| echo: FALSE
knitr::include_graphics(here("results","images","marsexample.jpeg"))
```
{{< pagebreak >}}

**KNN:**\
The KNN model predicts based on the closest samples or neighbors. Essentially, to predict a value, the data is broken up into samples/neighbors, and then the nearest samples (using Euclidean distance, typically)to the value of interest are examined to either classify or find a mean between the chosen samples. K represents the number of neighbors to utilize to come to this conclusion (@kuhn2018).

**Logistic Regression:**\
As mentioned earlier, Logistic Regression is similar to Linear Regression but the difference is that a Logistic Regression focuses on the probability of an event (p, p-1).

**RandomForest:**\
RandomForest models take advantage of decision trees. If we think about the scenario in our project (whether someone makes a deposit or not), we could imagine a decsion tree starting with, "after what age is someone more likely to subscribe?" This would be our first node to split the data on. We could continue asking things like, "Are those with housing loans more likely to subscribe?" or "Are those who are married more liklely to subscribe?", and these would represent more decision nodes for us to split the data on, getting us closer to the mode accurate prediction model. The RandomForest algorithm uses different methods to create several uncorrelated "forests" of decision trees(@IBM) .
{{< pagebreak >}}
### Background Regarding Performance Metrics

As highlighted in the data cleaning section, there were two data sets I was focusing on: one that had numeric factors for the categorical predictors, and one that had just numeric variables for the categorical predictors. I decided to examine both so there are a total of 8 different runs recorded below, 4 runs for each dataset.


**Description of Key Metrics:**\
**Accuracy:**\
The proportion of total correct classifications.\

**Recall:**\
The proportion of Positive cases that were correctly identified. This helps us understand how well we can classify positive cases specifically.\

**Precision:**\
The proportion of Positive classifications that were actually correct.\

**F1:**\
The harmonic mean of Precision and Recall. This metric helps iron out extreme values\
{{< pagebreak >}}


### Model Performance Metrics
@tbl-resulttable1 displays the relevant metrics for the models runs using the predictors that are numeric and are not factors.

```{r}
#| label: tbl-resulttable1
#| echo: FALSE
resulttable1 = readRDS(here("results","tables","finaltable1.rds"))
knitr::kable(resulttable1)
```

While all four models performed fairly similarly, the RandomForest model reported the highest values in all major categories. What stands out very clearly is that while all models have a fairly high accuracy scores, they also have very low recall values. We can deduce that this means the models are great at predicting negative cases (no subscription) but not very adequate at predicting positive cases (subscription). In the case of the Logistic Regression Model, there were 0 positive cases predicted at all, and thus the non-accuracy scores were 0 or NA.

@tbl-resulttable2 displays the relevant metrics for the models using the predictors that are factors (Age/Balance are still numeric).

```{r}
#| label: tbl-resulttable2
#| echo: FALSE
resulttable2 = readRDS(here("results","tables","finaltable2.rds"))
knitr::kable(resulttable2)
```

With these results we can see that the RandomForest Model is still the best performing, but in this case, the KNN model is not too far behind in most metrics. The Logistic Regression model performed better under these conditions, predicting some positive classes. However, the MARS model did not fare so well. In this run, the MARS model was the model to not identify a single positive class.
{{< pagebreak >}}

### Variables of Importance

Finally, we can examine which predictors specifically improved prediction the most. That is, which had the greatest weight on the final result. We will compare the difference between the two different runs of the same models Note: I determined it would only be helpful to include models that had more an 0 positive classes predicted.

@fig-result7 shows the Predictors that had the most impact on the Mars Model -- Numeric

```{r}
#| label: fig-result7
#| echo: FALSE
knitr::include_graphics(here("results","figures","importantM1.png"))
```

The numeric MARS models only highlighted a single variable of importance: Age.
{{< pagebreak >}}

@fig-result9 shows the Predictors that had the most impact on the KNN Model--Numeric

```{r}
#| label: fig-result9
#| echo: FALSE
knitr::include_graphics(here("results","figures","importantKNN1.png"))
```

Both KNN model runs returned the exact same most important factors, which were: Housing (whether someone had a housing loan or not), Balance (numeric value representing the customer's current account balance), and Education (Secondary, Tertiary, etc.).
{{< pagebreak >}}
@fig-result12 shows the Predictors that had the most impact on the Logistic Regression Model -- Factor

```{r}
#| label: fig-result12
#| fig-cap: "KNN Variables of Importance"
#| echo: FALSE
knitr::include_graphics(here("results","figures","importantLog2.png"))
```

Housing2 indicates that the person does have a housing loan and Loan2 indicates that the person has personal loan.Education3 indicates that the person has attained a tertiary level of education. Additionally we see different days being flagged as important in determining accuracy.
{{< pagebreak >}}
@fig-result13 shows the Predictors that had the most impact on the RF Model -- Numeric

```{r}
#| label: fig-result13
#| fig-cap: "RF Variables of Importance"
#| echo: FALSE
knitr::include_graphics(here("results","figures","importantRF1.png"))
```
{{< pagebreak >}}
@fig-result14 shows the Predictors that had the most impact on the RF Model -- Factor

```{r}
#| label: fig-result14
#| fig-cap: "KNN Variables of Importance"
#| echo: FALSE
knitr::include_graphics(here("results","figures","importantRF2.png"))
```

For both RandomForest models, Age and Housing were the variables with most impact on the model's prediction power. Balance and Day were next for both models in different order of importance.

{{< pagebreak >}}

# Discussion

## Summary and Interpretation

Initially the metrics produced by these models were showing substantial promise, but as I investigated the different confusion matrices I realized that the metrics were incorrectly labeling the negative classification as positive. When I reversed this option, the metrics very clearly shows that there was a drop in performance for predicting positive classifications. One explanation for this is that there were too few positive classes in the data as a whole. In the full data set, about 11% of the observations were positive cases, and this was also the case in the test data set as well. Given the size of the data set (45K records), and the small proportion of positive cases, there is reason to conclude that the models were victim to lack of familiarity with the positive cases.

  Despite these findings, the models were able to produce which variables provided the most weight towards the prediction power. The Housing variable was listed as the most important variable by every model with the exception of the Mars-Numeric model. Balance and Age were also listed as important by several different models. The variables that did not show up in the five important variables in any of the models were: Marital, Job, and Default. Education, Loan, and Day had a significant effect on some of the models and little to no effect on others.

## Conclusions

Using machine learning to better define a customer base can be an incredibly effective way to create more efficient marketing campaigns, as well as provide direction for better management of marketing resources.

  I attempted to develop a classification model that accurately predicted whether a customer had subscribed to a term deposit or not. While the different models that were ran initially showed potential, the predictive power of the best model was not at an ideal level. With a more balanced data set with similar variables, it is probable that a more precise model could have been produced. 
  
  The other goal of this project was to identify which variables could best aid in identifying customers that would subscribe to a term deposit. I found that there was quite a bit of variance among the predictors in terms of importance, with some predictors clearly standing out above the rest. This information could be used to better segment a consumer pool or adjust marketing tactics to prioritize customers displaying the variables of importance uncovered in this project. 
  
  Further research would include utilizing additional models to confirm the results presented here, incorporating a more balanced data set (as mentioned above), and exploring different tuning options than were used in this project.

{{< pagebreak >}}

# References
